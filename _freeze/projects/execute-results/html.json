{
  "hash": "8ec7c0e8ab9a204da04ded7c6337cd5e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Projects\"\ndate: last-modified\n---\n\n\n\n\n\n\n## Labs & Collaborators \n\nRob Goldstone - [Percepts and Concepts Lab](https://pc.cogs.indiana.edu/)\n\nRob Nosofsky - [Nosofsky Lab](https://nosofsky.cogs.indiana.edu/)\n\nChen Yu - [Developmental Intelligence Lab](https://www.la.utexas.edu/users/dil/)\n\nC. Shawn Green - [Learning and Transfer Lab](https://greenlab.psych.wisc.edu/)\n\n\n_______________________________________________________________\n\n\n\n\n\n\n\n\n## Primary PhD Work\n\nIn all of these projects, my goal is to understand and explain the patterns of performance observed, and to identify the cognitive and computational mechanisms that underlie learning and transfer. By exploring these topics, I hope to contribute to a better understanding of how we learn and generalize.\n\n[Link to working version of my dissertation](https://tegorman13.github.io/Dissertation/paper.html)\n\n### Variability and Visuomotor Learning\n\n[pdf of the journal\narticle](https://tegorman13.github.io/pdf/Gorman_Goldstone_2022_Instance-based_model_varied_practice.pdf){target=\"_blank\"}\\\n[Link to online version of journal\narticle](https://www.sciencedirect.com/science/article/abs/pii/S0010028522000299){target=\"_blank\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](projects_files/figure-html/unnamed-chunk-2-1.png){width=480}\n:::\n:::\n\n\n\n\nFor this project, I programmed a simple projectile launching task to serve as a conceptual replication of an influential paradigm in the visuomotor skill learning literature. Several of the canonical empirical patterns are replicated, with the varied trained participants tending to perform better during testing in both experiments. A major issue with previous research in the cross-disciplinary \"benefits of variability\" literature is that many previous works do not adequately control for the similarity between training and testing conditions. Such issues arise when both from failures to consider the possibility of non-linear generalization, and from often the unquestioned assumption that participants are acquiring, and then generalizing from prototype or schema-based representations. I introduce a theoretically motivated method of explicitly quantifying the similarity between training experience and testing condition. The resulting similarity quantity can then be used to explicitly control for similarity (by adding it as a covariate to the statistical model). The effect of variability remains significant while controlling for similarity, which I argue is a more rigorous demonstration of the effect of variability on testing performance than what is typically provided with standard methods. I conclude by introducing an extended version of the model that assumes training variation influences the steepness of the generalization gradient. With this flexible similarity mechanism, the group-level effect of variability can then be accounted for within the similarity-based generalization framework.\\\n\n\n### Examining the Effects of Training Variability on Extrapolation Performance\n\n[Link to project page](https://tegorman13.github.io/htw/){target=\"_blank\"} \\\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](projects_files/figure-html/unnamed-chunk-3-1.png){width=480}\n:::\n:::\n\n\n\n\nIn a follow up to my first project, a modified version of the task is used in conjunction with a testing procedure that challenges participants to extrapolate well beyond their training experience. In line with previous research in the function learning literature, participants show evidence of successful extrapolation in our linear task environment. Surprisingly though, the constant training group outperforms the varied training group consistently across numerous variants of the task. Such a pattern is far from unheard of in the vast literature on training variability, and it is therefore remains a worthwhile challenge to evaluate the ability of similarity-based models to account for the observed effects. Additionally, the cognitive process models implemented for project 2 will go beyond the modelling efforts of the previous project in two respects. 1) Extensions that enable the model to produce predictions of participant responses, and 2) fitting and attempting to account for behavior in both training AND testing phases of the experiment.\\\n\n\n### Investigating the Role of Variability in Real-World Learning: A Study Using Lost In Migration Data\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](projects_files/figure-html/unnamed-chunk-4-1.png){width=480}\n:::\n:::\n\n\n\n\nHere, I will take advantage of a large dataset from Lost In Migration, a popular online game resembling the classic Eriksen flanker task. Due to randomization of game trials, some users experience much more variability in the early phases of the game than others. Additionally, the dataset provided by Lumos labs includes data from users who experienced a 'high variability' version of the game before being switched to the standard version. I will assess the influence of both variability-via-randomization, and variability-via-manipulation, and once again evaluate the ability of similarity-based-generalization models to account for the observed patterns. While the key theoretical questions, and general modeling approach will remain the same as the first two projects, the Lost In Migration's dataset will present the opportunity to examine the influence of variability in a much more graded fashion, and to assess the ecological validity of the phenomena.\n\n\n## Other Projects\n\\\n\n### Learning the Structure of the Environment\n\\\n\n### Effect of Distance and Sequence in Category Learning\n\n  -  In Colloboration with [Bradley Rogers](https://www.linkedin.com/in/bradley-rogers-b618294)\n  - Presented at MathPsych/ICCM Conference (2018)\n\\\n\n### Skewed vs. Uniform Frequency Distributions in Cross-Situational Learning\n- Collobration with Chen Yu\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}