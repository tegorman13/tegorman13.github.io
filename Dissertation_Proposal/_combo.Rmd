---
title: "Dissertation Proposal"
subtitle: Thomas Gorman

output: 
  rmdformats::readthedown:
    toc_width: 1
csl: apa6.csl
bibliography: "HTW.bib"
knit: (function(input_file, encoding) {
    out_dir <- 'docs';
    rmarkdown::render(input_file,
      encoding=encoding,
      output_file=file.path(dirname(input_file), out_dir, 'combo.html'))})
editor_options: 
  markdown: 
    wrap: 72
pkgdown:
  as_is:true
---

```{r include = FALSE}
#my_citation <- cite_r(file = "r-references.bib")
#options(tinytex.verbose = TRUE)
source('HTW_Prep_Paper_Data.R')
#library(rmdformats)

# chains = T
# knitr::opts_chunk$set(warning = FALSE)
# knitr::opts_chunk$set(fig.pos = 'h')
```

# Project 1

**An Instance-Based Model Account of the Benefits of Varied Practice in Visuomotor Skill Learning**


## Abstract {.tabset}

Exposing learners to variability during training has been demonstrated to improve performance in subsequent transfer testing. Such variability benefits are often accounted for by assuming that learners are developing some general task schema or structure. However much of this research has neglected to account for differences in similarity between varied and constant training conditions. In a between-groups manipulation, we trained participants on a simple projectile launching task, with either varied or constant conditions. We replicate previous findings showing a transfer advantage of varied over constant training. Furthermore, we show that a standard similarity model is insufficient to account for the benefits of variation, but, if the model is adjusted to assume that varied learners are tuned towards a broader generalization gradient, then a similarity-based model is sufficient to explain the observed benefits of variation. Our results therefore suggest that some variability benefits can be accommodated within instance-based models without positing the learning of some schemata or structure. 


## Introduction {.tabset}


The past century of research on human learning has produced ample evidence that although learners can improve at almost any task, such improvements are often specific to the trained task, with unreliable or even nonexistent transfer to novel tasks or conditions  (Barnett & Ceci, 2002; Detterman, 1993). Such transfer challenges are of noteworthy practical relevance, given that educators, trainers, and rehabilitators typically intend for their students to be able to apply what they have learned to new situations. It is therefore important to better understand the factors that influence transfer, and to develop cognitive models that can predict when transfer is likely to occur. The factor of interest to the present investigation is variation during training. Our experiments add to the longstanding empirical investigation of the controversial relationship between training variation, and subsequent transfer. We also offer a novel explanation for such results in the form of an instance-based model that accounts for the benefits of variation in simple terms of psychological similarity. We first review the relevant concepts and literature. 

*1.1 Similarity and instance-based approaches to transfer of learning*

Notions of similarity have long played a central role in many prominent models of generalization of learning, as well as in the longstanding theoretical issue of whether learners abstract an aggregate, summary representation, or if they simply store individual instances. Early models of learning often assumed that discrete experiences with some task or category were not stored individually in memory, but instead promoted the formation of a summary representation, often referred to as a prototype or schema, and that exposure to novel examples would then prompt the retrieval of whichever preexisting prototype was most similar (Posner & Keele, 1968).  Prototype models were later challenged by the success of instance-based or exemplar models – which were shown to provide an account of generalization as good or better than prototype models, with the advantage of not assuming the explicit construction of an internal prototype (Estes, 1994; Hintzman, 1984; Medin & Schaffer, 1978; Nosofsky, 1986). Instance-based models assume that learners encode each experience with a task as a separate instance/exemplar/trace, and that each encoded trace is in turn compared against novel stimuli. As the number of stored instances increases, so does the likelihood that some previously stored instance will be retrieved to aid in the performance of a novel task. Stored instances are retrieved in the context of novel stimuli or tasks if they are sufficiently similar, thus suggesting that the process of computing similarity is of central importance to generalization. 

Similarity, defined in this literature as a function of psychological distance between instances or categories, has provided a successful account of generalization across numerous tasks and domains. In an influential study demonstrating an ordinal similarity effect, experimenters employed a numerosity judgment task in which participants quickly report the number of dots flashed on a screen. Performance (in terms of response times to new patterns) on novel dot configurations varied as an inverse function of their similarity to previously trained dot configurations (Palmeri, 1997). That is, performance was better on novel configurations moderately similar to trained configurations than to configurations with low-similarity, and also better on low-similarity configurations than to even less similar, unrelated configurations. Instance-based approaches have had some success accounting for performance in certain sub-domains of motor learning (Cohen & Rosenbaum, 2004; Crump & Logan, 2010; Meigh et al., 2018; Poldrack et al., 1999; Wifall et al., 2017) . Crump and Logan (2010) trained participants to type words on an unfamiliar keyboard, while constraining the letters composing the training words to a pre-specified letter set. Following training, typing speed was tested on previously experienced words composed of previously experienced letters; novel words composed of letters from the trained letter set; and novel words composed of letters from an untrained letter set. Consistent with an instance-based account, transfer performance was graded such that participants were fastest at typing the words they had previously trained on, followed by novel words composed of letters they had trained on, and slowest performance for new words composed of untrained letters.

*1.2 The effect of training variability on transfer*

While similarity-based models account for transfer by the degree of similarity between previous and new experiences, a largely separate body of research has focused on improving transfer by manipulating characteristics of the initial training stage. Such characteristics have included training difficulty, spacing, temporal order, feedback schedules, and the primary focus of the current work – variability of training examples. 

Research on the effects of varied training typically compares participants trained under constant, or minimal variability conditions to those trained from a variety of examples or conditions (Czyż, 2021; Soderstrom & Bjork, 2015). Varied training has been shown to influence learning in myriad domains including categorization of simple stimuli (Hahn et al., 2005; Maddox & Filoteo, 2011; Posner & Keele, 1968), complex categorization (Nosofsky et al., 2018), language learning (Jones & Brandt, 2020; Perry et al., 2010; Twomey et al., 2018; Wonnacott et al., 2012) anagram completion (Goode et al., 2008), trajectory extrapolation (Fulvio et al., 2014), task switching (Sabah et al., 2019), associative learning (Lee et al., 2019), visual search (George & Egner, 2021; Gonzalez & Madhavan, 2011; Kelley & Yantis, 2009), voice identity learning (Lavan et al., 2019), simple motor learning (Braun et al., 2009; Kerr & Booth, 1978; Roller et al., 2001; Willey & Liu, 2018), sports training (Green et al., 1995; North et al., 2019), and training on a complex video game (Seow et al., 2019). 

Training variation has received a particularly large amount of attention within the domain of visuomotor skill learning. Much of this research has been influenced by the work of Schmidt (1975), who proposed a schema-based account of motor learning as an attempt to address the longstanding problem of how novel movements are produced. According to Schema Theory, learners possess general motor programs for classes of movements (e.g. throwing a ball with an underhand movement), as well as schema rules that determine how a motor program is parameterized or scaled for a particular movement. Schema theory predicts that varied training results in the formation of a more general schema-rule, which can allow for transfer to novel movements within a given movement class. Experiments that test this hypothesis are often designed to compare the transfer performance of a constant-trained group against that of a varied-trained group. Both groups train on the same task, but the varied group practices from multiple levels of a task-relevant dimension that remains invariant for the constant group. For example, investigators might train two groups of participants to throw a projectile at a target, with a constant group that throws from a single location, and a varied group that throws from multiple locations. Both groups are then tested from novel locations.  Empirically observed benefits of the varied-trained group are then attributed to the variation they received during training, a finding observed in numerous studies (Catalano & Kleiner, 1984; Chua et al., 2019; Goodwin et al., 1998; Kerr & Booth, 1978; Wulf, 1991), and the benefits of this variation are typically thought to be mediated by the development of a more general schema for the throwing motion. 


Of course, the relationship between training variability and transfer is unlikely to be a simple function wherein increased variation is always beneficial. Numerous studies have found null, or in some cases negative effects of training variation (DeLosh et al., 1997; Sinkeviciute et al., 2019; Wrisberg et al., 1987), and many more have suggested that the benefits of variability may depend on additional factors such as prior task experience, the order of training trials,  or the type of transfer being measured (Berniker et al., 2014; Braithwaite & Goldstone, 2015; Hahn et al., 2005; Lavan et al., 2019; North et al., 2019; Sadakata & McQueen, 2014; Zaman et al., 2021). 



*Issues with Previous Research*

Although the benefits of training variation in visuomotor skill learning have been observed many times, null findings have also been repeatedly found, leading some researchers to question the veracity of the variability of practice hypothesis (Newell, 2003; Van Rossum, 1990). Critics have also pointed out that investigations of the effects of training variability, of the sort described above, often fail to control for the effect of similarity between training and testing conditions. For training tasks in which participants have numerous degrees of freedom (e.g. projectile throwing tasks where participants control the x and y velocity of the projectile), varied groups are likely to experience a wider range of the task space over the course of their training (e.g. more unique combinations of x and y velocities). Experimenters may attempt to account for this possibility by ensuring that the training location(s) of the varied and constant groups are an equal distance away from the eventual transfer locations, such that their training throws are, on average, equally similar to throws that would lead to good performance at the transfer locations. However, even this level of experimental control may still be insufficient to rule out the effect of similarity on transfer. Given that psychological similarity is typically best described as either a Gaussian or exponentially decaying function of psychological distance (Ennis et al., 1988; Ghahramani et al., 1996; Logan, 1988; Nosofsky, 1992; Shepard, 1987; Thoroughman & Taylor, 2005), it is plausible that a subset of the most similar training instances could have a disproportionate impact on generalization to transfer conditions, even if the average distance between training and transfer conditions is identical between groups. Figure 1 demonstrates the consequences of a generalization gradient that drops off as a Gaussian function of distance from training, as compared to a linear drop-off. 


## Methods {.tabset}

add methods

## Results {.tabset}

add results

## Discussion 

add discussion



# Project 2

## Introduction {.tabset}

In project 1, we applied model-based techniques to quantify and control
for the similarity between training and testing experience, which in
turn enabled us to account for the difference between varied and
constant training via an extended version of a similarity based
generalization model. In project 2, we will go a step further,
implementing a full process model capable of both 1) producing novel
responses and 2) modeling behavior in both the learning and testing
stages of the experiment. For this purpose, we will apply the
associative learning model (ALM) and the EXAM model of function learning
[@deloshExtrapolationSineQua1997]. ALM is a simple connectionist
learning model which closely resembles Kruschke's ALCOVE model
[@kruschkeALCOVEExemplarbasedConnectionist1992], with modifications to
allow for the generation of continuous responses.

Our research question here brings us into close alignment with the well
established literature on function learning
[@brehmerHypothesesRelationsScaled1974;
@carrollFunctionalLearningLearning1963;
@kohInductionCombinationRules1993]. Central questions of the function
learning literature include the capacity of learners to generalizing
what they have learned to novel instances that fall in-between or
outside prior experience, and the extent to which rule-based vs.
exemplar based association models can account for human behavior
[@bottNonmonotonicExtrapolationFunction2004;
@deloshExtrapolationSineQua1997; @jonesActiveFunctionLearning2018;
@kalishPopulationLinearExperts2004;
@mcdanielConceptualBasisFunction2005;
@mcdanielPredictingTransferPerformance2009].

## Methods

### [Participants]{.underline}

Data was collected from 647 participants (after exclusions). The results
shown below consider data from subjects in our initial experiment, which
consisted of 196 participants (106 constant, 90 varied). The follow-up
experiments entailed minor manipulations: 1) reversing the velocity
bands that were trained on vs. novel during testing; 2) providing
ordinal rather than numerical feedback during training (e.g. correct,
too low, too high). The data from these subsequent experiments are
largely consistently with our initial results shown below.

### [Task]{.underline}

We developed a novel visuomotor extrapolation task, termed the ["Hit The
Wall" (HTW]{.underline}) task, wherein participants learned to launch a
projectile such that it hit a rectangle at the far end of the screen
with an appropriate amount of force. Although the projectile had both x
and y velocity components, only the x-dimension was relevant for the
task.

[Link to task
demo](https://pcl.sitehost.iu.edu/tg/HTW/HTW_Index.html?sonaid=){target="_blank"}.

### [Design:]{.underline}

1)  90 training trials split evenly divided between velocity bands.
    Varied training with 3 velocity bands and Constant training with 1
    band.

2)  No-feedback testing from 3 novel extrapolation bands. 15 trials
    each.  

3)  No-feedbacd testing from the 3 bands used during the training phase
    (2 of which were novel for the constant group). 9 trials each.

4)  Feedback testing for each of the 3 extrapolation bands. 10 trials
    each.

## Results {.tabset}

### [Training:]{.underline}

Training performance is shown in Results Figure 2A. All groups show
improvement from each of their training velocity-bands (i.e. decreasing
average distance from target). In the velocity band trained at by both
groups (800-1000), the constant group maintains a superior level of
performance from the early through the final stages of training. This
difference is unsurprising given that the constant group had 3x more
practice trials from that band.

```{r Training, echo=FALSE,fig.height=5.0, fig.width=7}
#fig.cap="\\label{fig:figs}training performance"
nbins=8
dt%>% mutate(Trial.Bin=cut(trial,breaks=nbins,labels=FALSE)) %>%  
  group_by(sbjCode,condit,throwCategory,Trial.Bin) %>% summarise(dist=mean(dist),.groups = 'keep') %>%
   ggplot(aes(Trial.Bin,dist,color=throwCategory,group=throwCategory))+
  lineBars+
  facet_wrap(~condit)+
  scale_x_continuous(breaks=seq(1,nbins))+
  ylab("Mean Distance From Target Velocity")+xlab("Training Block")+
  scale_color_discrete(name="Velocity Band")+
  labs(title="Hit The Wall - Training Performance",
       caption="2A. Training Performance for both groups – binned into 8 blocks." )+
  theme(plot.caption=element_text(hjust=0,face="italic"),
        plot.title=element_text(face="bold"),
        axis.title.x=element_text(face="bold"),
        axis.title.y=element_text(face="bold"))

```

### [Testing:]{.underline}

Results Figure 2C shows the average velocity produced for all 6 bands
that were tested. At least at the aggregate level, both conditions were
able to differentiate all 6 bands in the correct order, despite only
having received training feedback for 1/6 (constant) or 3/6 (varied)
bands during training. Participants in both groups also had a bias
towards greatly overestimating the correct velocity for band 100-300,
for which both groups had an average of greater than 500.

```{r Testing Vx, echo=FALSE, fig.height=10, fig.width=12 }


sumStats = dtest %>% group_by(sbjCode,vbLabel,condit,throwCategory) %>%
  summarise(vxMean=mean(vxCapped),vxMedian=median(vxCapped),vxSd=sd(vxCapped),.groups = 'keep') %>%group_by(vbLabel,condit,throwCategory) %>%
  summarise(groupMean=round(mean(vxMean),0),groupMedian=round(mean(vxMedian),0),groupSd=round(mean(vxSd,na.rm=TRUE),0),.groups = 'keep') %>%
  mutate(meanLab=paste0("Mean=",groupMean),medianLab=paste0("Median=",groupMedian),sdLab=paste0("Sd=",groupSd)) %>%
  mutate(sumStatLab=paste0(meanLab,"\n",medianLab,"\n",sdLab))

fig2aCap=str_wrap("Figure 2B: Bands 100-300, 350-550 and 600-800 are novel extrapolations for both groups. Band 800-1000 was a training band for both groups. Bands 1000-1200, and 1200-1400 were trained for the varied group, and novel for the constant group.  Top figure displays mean deviation from correct velocity. Bottom figure displays the average % of trials where participants hit the wall with the correct velocity. Error bars indicate standard error of the mean. " ,width=200)

dtest %>% group_by(sbjCode,vbLabel,condit,throwCategory) %>%
  summarise(vxMean=mean(vxCapped),lowBound=first(bandInt),highBound=first(highBound),
            vbLag=first(vbLag),vbLead=first(vbLead),.groups = 'keep') %>%
  ggplot(aes(x=vbLabel,y=vxMean,fill=throwCategory))+
  geom_half_violin(color=NA)+ # remove border color
  geom_half_boxplot(position=position_nudge(x=-0.05),side="r",outlier.shape = NA,center=TRUE,
                    errorbar.draw = FALSE,width=.25)+
  geom_half_point(transformation = position_jitter(width = 0.05, height = 0.05),size=.3,aes(color=throwCategory))+
  facet_wrap(~condit,scale="free_x")+
  geom_rect(data=vbRect,aes(xmin=vbLag,xmax=vbLead,ymin=lowBound,ymax=highBound,fill=throwCategory),alpha=.3)+
  geom_text(data=sumStats,aes(y=2090,label = sumStatLab),size=2.5)+
  bandLines4+
  #geom_text(data=sumStats,aes(x=throwCategory,y=2100,label = groupMean),size=2, vjust = -0.5)+
  scale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2))+
  scale_fill_discrete(name="Velocity Band")+
  scale_color_discrete(guide="none")+  # remove extra legend
  theme(legend.position='none',
        plot.title=element_text(face="bold"),
        axis.title.x=element_text(face="bold"),
        axis.title.y=element_text(face="bold"),
        axis.text.x = element_text(size = 7.5))+
  ylab("Mean X Velocity")+xlab("Target Velocity Band") +
   labs(title="2B. Testing Performance (no-feedback) - X-Velocity Per Band",
       caption=fig2aCap)+
  theme(plot.caption=element_text(hjust=0,face="italic"))


```

For evaluating testing performance, we consider 3 separate metrics. 1)
The average absolute deviation from the correct velocity, 2) The % of
throws in which the wall was hit with the correct velocity and 3) The
average x velocity produced.

As is reflected in Results Figure 2B, the constant group performed
significantly better than the varied group at the 3 testing bands of
greatest interest. Both groups tended to perform worse for testing bands
further away from their training conditions. The varied group had a
slight advantage for bands 1000-1200 and 1200-1400, which were repeats
from training for the varied participants, but novel for the constant
participants.

```{r Test Performance, echo=FALSE, fig.height=9, fig.width=11}

gbDev<-dtest %>% group_by(sbjCode,vbLabel,condit,throwCategory) %>% 
  summarise(distMean=mean(distCapped),.groups = 'keep') %>% 
  mutate(meanDevCapped=ifelse(distMean>900,900,distMean)) %>%
  ggplot(aes(x=vbLabel,y=meanDevCapped,fill=condit))+
  stat_summary(geom="bar",fun=mean,position=dodge,alpha=.7)+
   stat_summary(geom="errorbar",fun.data=mean_se,alpha=.8,width=.5,position=dodge)+
 #ggbeeswarm::geom_quasirandom(aes(),dodge.width=.9,alpha=.15,size=.3)+
  scale_y_continuous(breaks=round(seq(0,1000,by=200),2))+
  ylab("Mean Absoulte Distance From Boundary")+xlab("Target Velocity Band") +
scale_fill_discrete(name="Training Condition",labels=c("Constant","Varied"))+scale_color_discrete(guide="none")+
  theme( plot.title=element_text(size=9),
        axis.title.x=element_text(face="bold",size=11),
        axis.title.y=element_text(face="bold",size=11),
        axis.text.x = element_text(size = 7.5),
        legend.position="top")+
  labs(title="")#Testing - Mean Absolute Distance From Boundary
leg=ggpubr::get_legend(gbDev)
gbDev<- gbDev+theme(legend.position='none')
  
gbHit<-dtest %>% group_by(sbjCode,condit,vbLabel,expMode,testMode) %>% 
  summarise(nHitsTest=sum(dist==0),n=n(),Percent_Hit=nHitsTest/n,.groups = 'keep') %>% 
  ggplot(aes(x=vbLabel,y=Percent_Hit,fill=condit))+
  stat_summary(geom="bar",fun=mean,position=dodge,alpha=.7)+
   stat_summary(geom="errorbar",fun.data=mean_se,alpha=.8,width=.5,position=dodge)+
# ggbeeswarm::geom_quasirandom(aes(),dodge.width=.9,alpha=.15,size=.3)+
  ylab("% of throws with correct velocity")+xlab("Target Velocity Band") + 
  scale_fill_discrete(guide='none')+scale_color_discrete(guide="none")+
  theme( plot.title=element_text(size=9),
        axis.title.x=element_text(face="bold",size=11),
        axis.title.y=element_text(face="bold",size=11),
        axis.text.x = element_text(size = 7.5),
        legend.position="top")+
  labs(title="")#Testing -% of hits



gtitle="2C. Testing Performance"
title = ggdraw()+draw_label(gtitle,fontface = 'bold',x=0,hjust=0)+theme(plot.margin = margin(0, 0, 0, 7))
captionText=str_wrap("Figure 2C: Bands 100-300, 350-550 and 600-800 are novel extrapolations for both groups. Band 800-1000 was a training band for both groups. Bands 1000-1200, and 1200-1400 were trained for the varied group, and novel for the constant group.  Right side figure displays mean deviation from correct velocity band (lower values correspond to better performance). Bottom Left displays the average % of trials where participants hit the wall with the correct velocity (higher values correspond got better performance). Error bars indicate standard error of the mean. ",200)
capt=ggdraw()+draw_label(captionText,fontface = 'italic',x=0,hjust=0,size=11)+theme(plot.margin = margin(0, 0, 0, 1))

plot_grid(title,NULL,leg,NULL,gbDev,gbHit,capt,NULL,ncol=2,rel_heights=c(.1,.1,1,.1),rel_widths=c(1,1))



```


### [Supplemental-Figures:]{.underline}


```{r Supp, echo=FALSE, fig.height=7, fig.width=9}



testRanks <- dtest %>% 
  group_by(sbjCode,condit,throwCategory) %>% 
  summarise(mDev=mean(dist),.groups = 'keep') %>% group_by(throwCategory,condit) %>% mutate(conditBandRank=rank(mDev)) %>%
  group_by(sbjCode) %>% mutate(rankAvg=mean(conditBandRank)) %>% group_by(sbjCode,condit) %>% summarise(rankAvg=first(rankAvg),.groups = 'keep') %>%
  group_by(condit) %>% mutate(msplit=factor(ntile(rankAvg,2),labels=c("High Performers","Low Performers")),
                              tsplit=factor(ntile(rankAvg,3),labels=c("High Performers","Mid Performers","Low Performers")),
                              fsplit=factor(ntile(rankAvg,4)),fiveSplit=factor(ntile(rankAvg,5)))


dtrainTest %>% merge(.,testRanks,by=c("sbjCode","condit")) %>% group_by(sbjCode,condit,phase,throwCategory) %>% 
   ggplot(aes(x=condit,y=vx,color=throwCategory,group=throwCategory))+
 lineBars+facet_grid(~phase)+
  ylab("vx")+xlab("Block")+
  ggtitle("Differentiation")+hgrid


```




## Modeling {.tabset}

In project 1, we applied model-based techniques to quantify and control
for the similarity between training and testing experience, which in
turn enabled us to account for the difference between varied and
constant training via an extended version of a similarity based
generalization model. In project 2, we will go a step further,
implementing a full process model capable of both 1) producing novel
responses and 2) modeling behavior in both the learning and testing
stages of the experiment. For this purpose, we will apply the
associative learning model (ALM) and the EXAM model of function learning
(DeLosh 1997). ALM is a simple connectionist learning model which
closely resembles Kruschke's ALCOVE model (Kruscke 1992), with
modifications to allow for the generation of continuous responses.

### [ALM & Exam Description]{.underline}

Delosh et al. (1997) introduced the associative learning model (ALM), a
connectionist model within the popular class of radial-basis networks.
ALM was inspired by, and closely resembles Kruschke's influential ALCOVE
model of categorization (Kruscke 1992). ALM is structured with input and
output nodes that correspond to regions of the stimulus space, and
response space, respectively. The units in the input layer activate as a
function of their similarity to a presented stimulus. As was the case
with the exemplar-based models, similarity in ALM is exponentially
decaying function of distance. So, for example, an input stimulus of
value 55 would induce maximal activation of the input unit tuned to 55.
Depending on the value of the generalization parameter, the nearby units
(e.g. 54 and 56; 53 and 57) may also activate to some degree. The input
layer is fully connected to the output layer, and the activation for any
particular output node is simply the weighted sum of the connection
weights between that node and the input activations. The network then
produces a response by taking the weighted average of the output units
(recall that each output unit has a value corresponding to a particular
response). During training, the network receives feedback which
activates each output unit as a function of its distance from the ideal
level of activation necessary to produce the correct response. The
connection weights between input and output units are then updated via
the standard delta learning rule, where the magnitude of weight changes
are controlled by a learning rate parameter.

See Table 2A for a full specification of the equations that define ALM
and EXAM.

### [Model Equations]{.underline}

```{r echo=FALSE}

text_tbl <- data.frame(
    'Step'=c("Input Activation","Output Activation","Output Probability","Mean Output","Feedback Activation","Update Weights","Extrapolation",""),
    'Equation' = c("$a_i$(X) = $\\frac{e^{-c \\cdot (X-X_i)^2}}{ \\sum_{k=1}^Me^{-c \\cdot (X-X_i)^2}}$", 
                   '$O_j$(X) = $\\sum_{k=1}^Mw_{ji} \\cdot a_i(X)$',
                   '$P[Y_j | X] = \\frac{O_i(X)}{\\sum_{k=1}^Mo_k(X)}$',
                   "$m(x) = \\sum_{j=1}^LY_j \\cdot \\bigg[\\frac{O_j(X)}{\\sum_{k=1}^Lo_k(X)}\\bigg]$",
                   "$f_j(Z)=e^{-c\\cdot(Z-Y_j)^2}$",
                   "$w_{ji}(t+1)=w_{ji}(t)+\\alpha \\cdot {f_i(Z(t))-O_j(X(t))} \\cdot a_i(X(t))$",
                   "$P[X_i|X] = \\frac{a_i(X)}{\\sum_{k=1}^Ma_k(X)}$",
                   "$E[Y|X_i]=m(X_i) + \\bigg[\\frac{m(X_{i+1})-m(X_{i-1})}{X_{i+1} - X_{i-1}} \\bigg] \\cdot[X-X_i]$"),
    
    'Description'= c(
            "Activation of each input node, $X_i$, is a function of the Gaussian similarity between the node value and stimulus X. ",
            "Activation of each Output unit $O_j$ is the weighted sum of the input activations and association weights",
            "Each output node has associated response, $Y_j$. The probability of response $Y_j$ is determined by the ratio of output activations",
            "The response to stimulus x is the weighted average of the response probabilities",
            "After responding, feedback signal Z is presented, activating each output node via the Gaussian similarity to the ideal response  ",
            "Delta rule to update weights. Magnitude of weight changes controlled by learning rate parameter alpha.",
            "Novel test stimulus X activates input nodes associated with trained stimuli",
            "Slope value computed from nearest training instances and then added to the response associated with the nearest training instance,m(x)")
)
text_tbl$Step=cell_spec(text_tbl$Step,font_size=12)
text_tbl$Equation=cell_spec(text_tbl$Equation,font_size=20)
almTable=kable(text_tbl, 'html', 
  booktabs=T, escape = F, align='l',
  caption = '<span style = "color:black;"><center><strong>Table 2A: ALM & EXAM Equations</strong></center></span>',
  col.names=c("","Equation","Description")) %>%
  kable_styling(position="left",bootstrap_options = c("hover")) %>%
  column_spec(1, bold = F,border_right=T) %>%
  column_spec(2, width = '10cm')%>%
  column_spec(3, width = '15cm') %>%
  pack_rows("ALM Activation & Response",1,4,bold=FALSE,italic=TRUE) %>%
  pack_rows("ALM Learning",5,6,bold=FALSE,italic=TRUE) %>%
  pack_rows("EXAM",7,8,bold=FALSE,italic=TRUE) 
  #save_kable(file="almTable.html",self_contained=T)
almTable



```

### [Model Fitting]{.underline}

Following the procedure used by McDaniel & Busemeyer (2009), we will
assess the ability of both ALM and EXAM to account for the empirical
data when fitting the models to 1) only the training data, and 2) both
training and testing data. Models will be fit directly to the trial by
trial data of each individual participants, both by minimizing the
root-mean squared deviation (RMSE), and by maximizing log likelihood.

Finding the sensitivity and learning rate parameters most likely to have
generated the training data from a given participant. Each set of
parameter values results in a learning trajectory that produces a weight
matrix.


# Project 3

## Background

Last year, we applied to access data generated from the many popular online games by Lumos Labs (Lumosity). We received access to data from “Lost in Migration” a gamified version of the Eriksen Flanker task. Users are presented with a configuration of 5 “birds” - a central “target” bird and 4 “flanker” birds. The task is to indicate the direction of the central bird as fast as possible using the up/down/right/left arrow keys on their device. On 50% of trials, the flanker birds are pointing the same direction as the target birds, while on the other half of trials the flanker birds point in an incompatible direction. 

The total dataset consists of 8,551 individual users, 168,307 game sessions, and 8,207,980 trials of the game. A subset of the users in the dataset participated in a “split-test” conducted by Lumos Labs, wherein new users could be randomly assigned to one of several modified version of the game. The modified versions all increased the range of possible values for up to 3 aspects of the task stimuli: 
1) The rotation of the flanker birds relative to the target on incongruent trials (90,180,270 in standard version, continuous range in modified version)
2) The distance between flankers and target (constant in standard version), 
3) The size of the stimuli (constant in standard version)


## Proposed Analysis 1 {.tabset}

**Effect of split-test variability manipulation on learning** 1)
Residual Effect of Split-testvariation

-   After split-test users return to the standard version of LIM, for
    how long will we be able to detect behavioral differences between
    split-test and standard-only users (in terms of games and/or
    trials).

2)  Between-Group comparisons

-Standard version users vs. split test users during 'training' (Figure
3D)

-Standard vs. split-test on the first game after split test users switch
to standard version

o  Interaction with how many games split test users completed before
switching

-Higher varied vs. lower varied split test users

o  E.g'Flanker Size' users vs. 'Flanker Size & Rotation & Distance'
users.

## Proposed Analysis 2 {.tabset}

**Trial-by-trial influence of repetition and variability** Like many
in-lab experiments, the LIM trials are randomized. A consequence of this
randomization is that some users will experience a greater proportion of
repeat trials, while others.

Trial-by-trial variability

-   The total \# of unique configurations experienced

-   The distance between trials - in terms of properties of stimuli
(number, arrangement)

- Distance between a new trial, and all of the previously experienced
trials (or some window).



## References {.tabset}
